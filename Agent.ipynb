{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size, frame_size=1, state_dict=None, target_update_interval=50, train=True):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.episode = 0\n",
    "\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.000005\n",
    "        self.eps = 1.0 if train else 0.00001\n",
    "        self.eps_decay_rate = 0.999\n",
    "        self.eps_min = 0.05\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.target_model = DQN(state_size, action_size)\n",
    "        if state_dict is not None:\n",
    "            self.model.load_state_dict(state_dict)\n",
    "            self.target_model.load_state_dict(state_dict)\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def get_action(self, x, mask):\n",
    "        if self.eps_min < self.eps:\n",
    "            self.eps *= self.eps_decay_rate\n",
    "        if np.random.rand() <= self.eps:\n",
    "            return random.choice(np.arange(self.action_size)[mask])\n",
    "        else:\n",
    "            x = self.preprocess_state(x)\n",
    "            x = torch.FloatTensor(x)\n",
    "            x = self.model(x.view(1, self.state_size))\n",
    "            x = x * mask\n",
    "            return int(torch.argmax(x))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        # new_state = state[[0, 1, 3, 4, 6, 7, 9, 10], :]\n",
    "        return state\n",
    "    \n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        state = torch.FloatTensor(self.preprocess_state(state)).view(self.state_size)\n",
    "        next_state = torch.FloatTensor(self.preprocess_state(next_state)).view(self.state_size)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            self.episode += 1\n",
    "    \n",
    "    def train_model(self):\n",
    "        if len(self.memory) < 2000:\n",
    "            return\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = torch.zeros((self.batch_size, self.state_size))\n",
    "        next_states = torch.zeros((self.batch_size, self.state_size))\n",
    "        actions = torch.zeros(self.batch_size).type(torch.LongTensor)\n",
    "        rewards = torch.zeros(self.batch_size)\n",
    "        dones = torch.zeros(self.batch_size).type(torch.LongTensor)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = mini_batch[i][0]\n",
    "            actions[i] = int(mini_batch[i][1])\n",
    "            rewards[i] = float(mini_batch[i][2])\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            dones[i] = mini_batch[i][4]\n",
    "\n",
    "        next_q_val = self.target_model(next_states)\n",
    "        q_val = self.model(states)\n",
    "        exp_q_val = torch.zeros_like(q_val)\n",
    "\n",
    "        terminal = torch.where(dones != 0)[0]\n",
    "        not_terminal = torch.where(dones == 0)[0]\n",
    "\n",
    "        if terminal.numel() > 0:\n",
    "            exp_q_val[terminal, actions[terminal]] = rewards[terminal]\n",
    "        if not_terminal.numel() > 0:\n",
    "            exp_q_val[not_terminal, actions[not_terminal]] = rewards[not_terminal] + \\\n",
    "                self.discount_factor * torch.max(next_q_val[not_terminal, :], axis=1).values\n",
    "\n",
    "        loss = self.loss(q_val, exp_q_val)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        if self.episode % self.target_update_interval == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "    def get_mask(self, env):\n",
    "        mask = np.array([False]*23)\n",
    "        if env.check == 1:\n",
    "            mask[0:4] = True\n",
    "            if env.prev_action in range(0,4): # 만약 이전에도 change 였다면\n",
    "                mask[:] = False\n",
    "                mask[env.prev_action] = True #계속 해라\n",
    "        if env.change == 1:\n",
    "            mask[4:8] = True # Change 다 킨다.\n",
    "            mask[env.process_mode + 4] = False # 같은 process mode로는 불가능하므로 끈다.\n",
    "            if env.prev_action in range(4,8): # 만약 이전에도 change 였다면\n",
    "                mask[:] = False\n",
    "                mask[env.prev_action] = True #계속 해라\n",
    "        if env.stop == 1:\n",
    "            mask[8] = True\n",
    "        if env.process == 1:\n",
    "            mask[9:] = True\n",
    "            if env.step_count <= 554:\n",
    "                mask[10:] = False\n",
    "            for x1, x2 in zip(np.arange(140, 133.5, -0.5), np.arange(9, 22)):\n",
    "                if env.day_process_n >= x1:\n",
    "                    mask[x2+1:] = False\n",
    "        return mask\n",
    "\n",
    "        \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer = layer = nn.Sequential(\n",
    "            nn.Linear(state_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
